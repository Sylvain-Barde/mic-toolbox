{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIC Demo 2 - Parallelising multiple measurements\n",
    "\n",
    "As the first part of the demonstration 1 shows, obtaining a multivariate MIC measurement requires multiple runs of the MIC algorithm. This is because:\n",
    "- A multivariate measurement on $K$ variables is  decomposed into a sum of $K$ univariate runs, where each variable is suitably conditioned on the other contemporaneous variables.\n",
    "- In order to improve accuracy, multiple measurements should be carried out using different conditioning orders for the decomposition and then the average should be taken.\n",
    "- Finally, because the purpose of the MIC is model comparison, this will need to be carried out for multiple models.\n",
    "\n",
    "As a result, the obvious solution to this is to parallelise the runs. This can be done using the `multiprocessing` package, as described below, on the same dataset as used in the Demo 1 file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import mic.toolbox as mt\n",
    "import multiprocessing as mp\n",
    "\n",
    "import wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to running the multivariate MIC, the steps required for a single run are bundled into a simple wrapper function `wrapper.py`. Note, when running in python from the console, this function can be in the same file as the multiprocessing call below, however, in Jupyter the function [has to be imported to work properly](https://medium.com/@grvsinghal/speed-up-your-python-code-using-multiprocessing-on-windows-and-jupyter-or-ipython-2714b49d6fac).\n",
    "\n",
    "As can be noted from the prinout below, the function contains the same steps as outlined in the Demo 1 file, the only additions being the unpacking/packing of inputs/outputs and the logging of individual tasks to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import sys\n",
      "import time\n",
      "import numpy as np\n",
      "import mic.toolbox as mt\n",
      "import multiprocessing as mp\n",
      "\n",
      "def MIC_wrapper(inputs):\n",
      "    \"\"\" wrapper function\"\"\"\n",
      "\n",
      "    tic = time.time()\n",
      "\n",
      "    # Unpack inputs and parameters\n",
      "    params = inputs[0]\n",
      "    model = inputs[1]\n",
      "    var_vec = inputs[2]\n",
      "    num = inputs[3]\n",
      "    \n",
      "    log_path = params['log_path']\n",
      "    data_path = params['data_path']\n",
      "    lb = params['lb']\n",
      "    ub = params['ub']\n",
      "    r_vec = params['r_vec']\n",
      "    hp_bit_vec = params['hp_bit_vec']\n",
      "    mem = params['mem']\n",
      "    d = params['d']\n",
      "    lags = params['lags']\n",
      "    \n",
      "    print (' Task number {:2d} initialised'.format(num))\n",
      "\n",
      "    # Redirect output to file and print task/process information\n",
      "    main_stdout = sys.stdout\n",
      "    sys.stdout = open(log_path + '//log_' + str(num) + '.out', \"w\")\n",
      "    print (' Task number :   {:3d}'.format(num))\n",
      "    print (' Parent process: {:10d}'.format(os.getppid()))\n",
      "    print (' Process id:     {:10d}'.format(os.getpid()))    \n",
      "    \n",
      "    # Load simulates/empirical data\n",
      "    sim_data = np.loadtxt(model, delimiter=\"\\t\") \n",
      "    emp_data = np.loadtxt(data_path, delimiter=\"\\t\")\n",
      "    \n",
      "    # Pick a tag for the tree (useful for indentifying the tree later on)\n",
      "    tag = model\n",
      "    \n",
      "    # -- Stage 1\n",
      "    # -- Generate permutation from data\n",
      "    perm = mt.corr_perm(emp_data[:,0:2], r_vec, hp_bit_vec, var_vec, lags, d)\n",
      "    \n",
      "    # Discretise the training data. \n",
      "    sim_dat1 = sim_data[:,0:2]\n",
      "    data_struct = mt.bin_quant(sim_dat1,lb,ub,r_vec,'notests') # Note the 'notests' option\n",
      "    data_bin = data_struct['binary_data']\n",
      "\n",
      "    # Initialise a tree and train it, trying to predict the 1st variable\n",
      "    var = var_vec[0]\n",
      "    output = mt.train(None, data_bin, mem, lags, d, var, tag, perm)\n",
      "    \n",
      "    # Discretise the second run of training data\n",
      "    sim_dat1 = sim_data[:,2:4]\n",
      "    data_struct = mt.bin_quant(sim_dat1,lb,ub,r_vec,'notests') # Note, we are not running discretisation tests\n",
      "    data_bin = data_struct['binary_data']\n",
      "\n",
      "    # Extract the tree from the previous output and train it again. Only the 1st argument changes\n",
      "    T = output['T']\n",
      "    output = mt.train(T, data_bin, mem, lags, d, var, tag, perm)\n",
      "    \n",
      "    # Use the built-in descriptive statistic method to get some diagnostics\n",
      "    T = output['T']\n",
      "    T.desc()\n",
      "    \n",
      "    # -- Stage 2\n",
      "    # Score the empirical data with the model probabilities\n",
      "    scores = np.zeros([998,10])\n",
      "\n",
      "    for j in range(10):\n",
      "        loop_t = time.time()\n",
      "\n",
      "        # Discretise the data\n",
      "        k = 2*j\n",
      "        dat = emp_data[:,k:k+2]\n",
      "        data_struct_emp = mt.bin_quant(dat,lb,ub,T.r_vec,'notests')\n",
      "        data_bin_emp = data_struct_emp['binary_data']\n",
      "\n",
      "        # Score the data using the tree\n",
      "        score_struct = mt.score(T, data_bin_emp)\n",
      "\n",
      "        # Correct the measurement\n",
      "        scores[:,j] = score_struct['score'] - score_struct['bound_corr']\n",
      "\n",
      "        print('Replication {:2d}: {:10.4f} secs.'.format(j,time.time() - loop_t))\n",
      "        \n",
      "    # Redirect output to console and print completetion time\n",
      "    sys.stdout = main_stdout\n",
      "    toc = time.time() - tic\n",
      "    print (' Task number {:3d} complete - {:10.4f} secs.'.format(int(num),toc))\n",
      "\n",
      "    # Return scores\n",
      "    return (scores)\n"
     ]
    }
   ],
   "source": [
    "print(open('wrapper.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this wrapper file is written we can use the `multiprocessing` package to run the same steps on different simulated data and conditioning orders. Note, we are re-using the discretisation settings we used in the Demo 1 file. In a more general application, one would need to make sure that the correct discretisation and memory settings have been found via a small scale exploratory run prior to launching the full analysis.\n",
    "\n",
    "We create a directory in order to be able to record the output of individual runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for saving logs of individual tasks\n",
    "log_path = 'logs'\n",
    "if not os.path.exists(log_path):\n",
    "    os.makedirs(log_path,mode=0o777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now setup the parallel run. We start by setting the common parameters for the run. These are:\n",
    "- The location of the empirical data and logging folder.\n",
    "- The discretisation settings `lb`, `ub`, `r_vec` and `hp_bit_vec`.\n",
    "- The tree and memory settings `mem`, `d` and `lags`.\n",
    "\n",
    "We then create a list of tuples parametrising each run. In this case, these contain:\n",
    "- The conditioning order needed for the measurements\n",
    "- The simulated datasets corresponding to each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set common parameters\n",
    "params = dict(log_path = log_path,\n",
    "            data_path = 'data/emp_data.txt',\n",
    "            lb = [-10,-10],\n",
    "            ub = [ 10, 10],\n",
    "            r_vec = [7,7],\n",
    "            hp_bit_vec = [3,3],\n",
    "            mem = 200000,\n",
    "            d = 24,\n",
    "            lags = 2)\n",
    "\n",
    "# List all the possible conditioning orders and datasets \n",
    "var_vecs = [[1,2],\n",
    "           [2],\n",
    "           [2,1],\n",
    "           [1]]\n",
    "\n",
    "models = ['data/model_1.txt',\n",
    "          'data/model_2.txt']\n",
    "\n",
    "# Populate job (a list) using tuples.\n",
    "job_inputs = []\n",
    "count = 1\n",
    "for model in models:\n",
    "    for var_vec in var_vecs:\n",
    "        job_inputs.append((params, model, var_vec, count))\n",
    "        count += 1\n",
    "        \n",
    "num_tasks = len(job_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use a multiprocessing pool to run the list of jobs in parallel. This simple example can be run on a simple quad core processor, however it can easily be extended to more cores for a larger number of variables $K$ and/or models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel job complete\n",
      "Elapsed time -   369.5989 secs.\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Set cores - assuming a standard quadcore PC here. \n",
    "    num_cores = 4\n",
    "\n",
    "    # Create pool and run parallel job\n",
    "    pool = mp.Pool(processes=num_cores)\n",
    "    results = pool.map(wrapper.MIC_wrapper,job_inputs)\n",
    "\n",
    "    # Close pool when job is done\n",
    "    pool.close()\n",
    "\n",
    "print('Parallel job complete')\n",
    "print('Elapsed time - {:10.4f} secs.'.format(time.time() - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the parallel run is over, the measurements can be added and averaged to generate the MIC score for each model on each replication. For each model $j$ (1 or 2) the MIC measurement is the average of the two possible ways of decomposing the cross-entropy of a bi-variate system:\n",
    "\n",
    "$$ \\lambda^j (X) = \\frac{1}{2}\\sum _{t=L}^T \\left[ \\lambda^j (x_t^1 \\mid x_t^2, \\Omega_t) + \\lambda^j (x_t^2 \\mid \\Omega_t) \\right] + \\frac{1}{2}\\sum _{t=L}^T \\left[ \\lambda^j (x_t^2 \\mid x_t^1, \\Omega_t) + \\lambda^j (x_t^1 \\mid \\Omega_t) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MIC scores obtained for Model 1:\n",
      "    9649.43    9698.55    9663.38    9637.58    9644.74    9677.28    9669.60    9679.73    9685.27    9608.37\n",
      "\n",
      " MIC scores obtained for Model 2:\n",
      "    9582.25    9651.00    9629.27    9620.19    9627.32    9617.76    9636.36    9634.48    9641.34    9544.12\n",
      "\n",
      "\n",
      " Difference in MIC (Model 2 - Model 1):\n",
      "     -67.18     -47.56     -34.11     -17.39     -17.41     -59.51     -33.23     -45.25     -43.93     -64.26\n",
      "\n",
      " Standard deviation (Model 2 - Model 1):\n",
      "       0.57       0.53       0.55       0.53       0.54       0.53       0.51       0.52       0.56       0.48\n",
      "\n",
      " T statistic (Model 2 - Model 1):\n",
      "    -116.97     -90.36     -61.90     -33.00     -32.11    -111.94     -65.13     -86.32     -78.89    -133.56\n"
     ]
    }
   ],
   "source": [
    "mic = np.zeros([2,998,10])\n",
    "mic_diff = np.zeros([998,10])\n",
    "task = 0\n",
    "\n",
    "# Add and average measurements for the two models\n",
    "for model in range(2):\n",
    "    for cond_order in range(4):\n",
    "        mic[model,:,:] += 0.5*results[task]\n",
    "        task += 1\n",
    "\n",
    "# Calculate the difference across models, including t-statistic\n",
    "mic_diff[:,:] = mic[1,:,:] - mic[0,:,:]\n",
    "mic_diff_v = np.var(mic_diff,0,keepdims = False)\n",
    "t_stat = np.sum(mic_diff,0)/(mic_diff_v**0.5)\n",
    "\n",
    "# Print the results\n",
    "flt_str = '    {:7.2f}'*10    \n",
    "print('\\n MIC scores obtained for Model 1:')\n",
    "print(flt_str.format(*np.sum(mic[0,:,:],0)))\n",
    "print('\\n MIC scores obtained for Model 2:')\n",
    "print(flt_str.format(*np.sum(mic[1,:,:],0)))\n",
    "print('\\n\\n Difference in MIC (Model 2 - Model 1):')\n",
    "print(flt_str.format(*np.sum(mic_diff,0)))\n",
    "print('\\n Standard deviation (Model 2 - Model 1):')\n",
    "print(flt_str.format(*(mic_diff_v**0.5)))\n",
    "print('\\n T statistic (Model 2 - Model 1):')\n",
    "print(flt_str.format(*(t_stat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would conclude from this exercise that model 2 is the better model as it has a lower MIC score than model 1, and the mean difference seems significant across repilications.\n",
    "\n",
    "Note that the T-statistic for the difference in means is used here to illustrate that statistical testing is feasilbe as the MIC is provided at the observation level. This assumes that the distribution is normal, however, which is not the case. It is therefore preferable in general to use bootstrap based methods such as the [Model Confidence Set of Hansen et al. 2011](https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA5771)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
